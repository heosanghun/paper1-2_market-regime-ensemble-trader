## 시장 레짐에 대응하는 강건한 동적 앙상블 강화학습 트레이딩 시스템

**허상훈*** · **황영배**\*\*

\*충북대학교 산업인공지능학과  
(sanhun.heo@chungbuk.ac.kr)  
\*\*충북대학교 산업인공지능학과  
(ybhwang@cbnu.ac.kr)

---

**초록 (Abstract)**

본 연구는 금융 시장의 내재적인 비정형성과 다양한 시장 레짐(regime) 변화에 효과적으로 대응하여 장기적으로 안정적인 수익을 창출하는 것을 목표로 하는 동적 앙상블 강화학습 트레이딩 시스템을 제안한다. 기존 연구들은 시장 레짐 식별 자체에 집중하거나, 식별된 레짐 정보를 트레이딩 전략으로 효과적으로 변환하는 데 한계를 보여왔다. 제안하는 시스템은 멀티모달 데이터(캔들차트, 기술적 지표, 뉴스 감성 점수)를 활용하여 시장 레짐을 실시간으로 분류하고, 각 레짐(상승장, 하락장, 횡보장)에 특화된 강화학습 에이전트 풀을 구성한다. 이후, 동적 앙상블 메커니즘을 통해 시장 상황에 따라 최적의 에이전트 조합과 가중치를 할당하여 최종 투자 결정을 내린다. 비트코인(BTC/USDT) 시계열 데이터(2019-2023년)를 사용한 백테스팅 결과, 제안된 동적 앙상블 시스템은 정적 멀티모달 시스템 대비 샤프 비율(Sharpe Ratio) 약 0.4-0.6 증가, 최대 낙폭(MDD) 약 7-9%p 감소, 승률(Win Rate) 약 5-8%p 향상 등 주요 성과 지표에서 통계적으로 유의미한 우위를 나타냈다. 또한, 평균 84.5%의 시장 레짐 감지 정확도를 달성하며, 이를 통해 불필요한 거래를 줄여 거래 비용을 약 12-15% 절감하는 효과를 보였다. 본 연구는 시장 레짐 변화에 대한 적응성을 높이고, 강화학습 기반 트레이딩 시스템의 실용성을 제고하는 데 기여할 것으로 기대된다.

**주제어**: 강화학습, 동적 앙상블, 시장 레짐, 멀티모달 학습, 금융 시계열 분석, 트레이딩 시스템

---

**1. 서론 (Introduction)**

금융 시장은 본질적으로 높은 변동성과 비선형성을 특징으로 하며, 거시 경제 상황, 정책 변화, 투자자 심리 등 다양한 요인에 의해 시장의 특징적 상태, 즉 시장 레짐(market regime)이 변화한다 (Ang & Bekaert, 2002). 이러한 시장 레짐은 일반적으로 상승장(bull market), 하락장(bear market), 횡보장(sideways market) 등으로 구분되며, 각 레짐에 따라 효과적인 투자 전략 또한 달라진다. 그러나 기존의 많은 트레이딩 시스템은 특정 시장 상황에만 강점을 보이거나, 레짐 변화에 대한 적응력이 부족하여 장기적으로 안정적인 성과를 내는 데 어려움을 겪는다.

최근 인공지능, 특히 강화학습(Reinforcement Learning, RL) 기법이 금융 트레이딩 분야에 활발히 적용되고 있으나 ( 예를 들어, Deng et al., 2017; Xiong et al., 2018), 대부분의 연구는 단일 모델을 사용하거나 시장 레짐의 변화를 명시적으로 고려하지 않는 경우가 많다. 일부 연구들은 시장 레짐을 식별하는 데 초점을 맞추고 있으나 (예: Hamilton, 1989; Kritzman et al., 2012), 식별된 레짐 정보를 실제 트레이딩 전략으로 효과적으로 통합하고, 각 레짐에 최적화된 행동을 동적으로 선택하는 메커니즘은 여전히 중요한 연구 과제로 남아있다.

본 연구는 이러한 한계점을 극복하고자, 다양한 시장 레짐 변화에 포괄적으로 대응하고 장기적으로 안정적인 수익 달성을 목표로 하는 동적 앙상블 강화학습 트레이딩 시스템을 제안한다. 제안 시스템은 먼저 멀티모달 데이터(캔들차트, 기술적 지표, 뉴스 텍스트)를 활용하여 현재 시장 레짐을 정확하게 분류한다. 이후, 각 레짐(상승장, 하락장, 횡보장)에 특화되어 사전 학습된 다수의 강화학습 에이전트들로 구성된 풀(pool)에서, 현재 레짐 및 에이전트 간 상관관계를 고려한 동적 가중치 할당 메커니즘을 통해 최적의 에이전트 조합을 구성하고 최종 투자 결정을 수행한다.

비트코인(BTC/USDT)의 2019년부터 2023년까지의 시계열 데이터를 이용한 백테스팅 결과, 제안된 동적 앙상블 시스템은 시장 레짐을 고려하지 않는 정적 멀티모달 시스템과 비교하여 샤프 비율(Sharpe Ratio)이 약 0.4-0.6 증가하였고, 최대 낙폭(Maximum Drawdown, MDD)은 약 7-9%p 감소하였으며, 승률(Win Rate)은 약 5-8%p 향상되는 등 주요 성과 지표에서 통계적으로 유의미한 우위를 보였다. 특히, 시장 레짐 감지 정확도는 평균 84.5% 수준을 달성하였으며, 이는 거래 비용 최적화에 기여하여 불필요한 거래 비용을 약 12-15% 절감하는 효과로 이어졌다.

본 논문의 구성은 다음과 같다. 2장에서는 시장 레짐 분석, 금융 분야의 앙상블 학습, 강화학습의 금융 시장 적용에 대한 관련 연구를 살펴본다. 3장에서는 제안하는 동적 앙상블 강화학습 트레이딩 시스템의 전체 아키텍처와 각 구성 요소(시장 레짐 분류, 강화학습 에이전트 설계, 동적 앙상블 메커니즘)에 대해 상세히 설명한다. 4장에서는 실험 데이터셋, 비교 대상 모델, 평가 지표 등 실험 설계를 기술하고, 5장에서는 실험 결과를 제시하고 심층적으로 분석한다. 마지막으로 6장에서는 연구 결과를 요약하고, 학술적 및 실무적 시사점, 연구의 한계점, 그리고 향후 연구 방향을 논의하며 결론을 맺는다.

**2. 이론적 배경 (Related Work)**

**2.1. 시장 레짐 분석 및 예측**
금융 시장 레짐은 특정 기간 동안 시장이 나타내는 특징적인 상태 또는 조건을 의미하며, 전통적으로는 시장의 추세(trend) 방향에 따라 상승장, 하락장, 횡보장으로 구분된다 (Fabozzi & Markowitz, 2011). 시장 레짐을 정확히 식별하고 예측하는 것은 자산 배분, 위험 관리, 트레이딩 전략 수립에 있어 매우 중요하다.

레짐 식별을 위한 접근법은 크게 규칙 기반 방식과 통계/기계학습 기반 방식으로 나눌 수 있다. 규칙 기반 방식은 이동평균선, 특정 가격 패턴 등 사전에 정의된 규칙을 사용하여 레짐을 구분하지만, 시장의 복잡성과 동적인 변화를 충분히 반영하기 어렵다는 단점이 있다.

통계 기반 방식으로는 Hamilton (1989)이 제안한 마르코프 전환 모델(Markov Switching Model)이 대표적이다. 이 모델은 관찰되지 않는 상태 변수(레짐)가 마르코프 과정을 따른다고 가정하고, 시계열 데이터로부터 레짐 전환 확률과 각 레짐의 특성을 추정한다. 이후 많은 연구에서 이 모델을 확장하여 금융 시장 분석에 활용해왔다 (Ang & Bekaert, 2002).

최근 머신러닝 기술의 발전은 비선형적이고 복잡한 패턴 인식을 가능하게 함으로써 레짐 분석에 새로운 가능성을 열었다. 은닉 마르코프 모델(Hidden Markov Model, HMM), 서포트 벡터 머신(Support Vector Machine, SVM), 그리고 다양한 딥러닝 기법(예: 순환 신경망(Recurrent Neural Network, RNN), 컨볼루션 신경망(Convolutional Neural Network, CNN))들이 시장 레짐 분류 및 예측에 적용되고 있다 (Kritzman et al., 2012; Gu et al., 2020). 또한, 금융 뉴스나 소셜 미디어 텍스트에서 감성 정보나 토픽을 추출하는 자연어 처리(Natural Language Processing, NLP) 알고리즘을 활용하여 시장 심리를 파악하고 이를 레짐 분석에 통합하려는 시도도 이루어지고 있다 (Kearney & Liu, 2014).

**2.2. 금융 분야에서의 앙상블 학습**
앙상블 학습(ensemble learning)은 여러 개별 모델(base learners)의 예측을 결합하여 단일 모델보다 더 우수하고 강건한 성능을 달성하고자 하는 기계학습 기법이다 (Dietterich, 2000). 금융 시장의 예측 불가능성과 노이즈가 많은 데이터 특성으로 인해, 단일 모델보다는 앙상블 기법이 더 안정적인 결과를 제공하는 경우가 많다. 주요 앙상블 방식으로는 배깅(bagging), 부스팅(boosting), 스태킹(stacking) 등이 있다.

특히 본 연구와 밀접하게 관련된 접근법은 동적 앙상블(dynamic ensemble) 또는 적응형 앙상블(adaptive ensemble) 기법이다. 이는 정적으로 모델을 결합하는 대신, 시간의 흐름이나 특정 상황 변화에 따라 각 구성 모델의 가중치나 선택 여부를 동적으로 조절하는 접근법이다 (Kuncheva, 2004). 금융 시장은 그 특성이 시간에 따라 끊임없이 변하는 비정형성(non-stationarity)을 가지므로, 이러한 동적 앙상블 기법은 시장 변화에 보다 유연하게 대응할 수 있는 잠재력을 지닌다. Timmermann (2006)은 경제 예측에서 다양한 모델 예측을 결합하는 이론적 근거와 방법론을 제시하였으며, 이는 동적 가중치 할당의 기초가 될 수 있다.

**2.3. 강화학습의 금융 시장 적용**
강화학습(Reinforcement Learning, RL)은 에이전트(agent)가 환경(environment)과의 상호작용을 통해 시행착오를 겪으며 누적 보상(reward)을 최대화하는 행동 정책(policy)을 학습하는 머신러닝의 한 패러다임이다 (Sutton & Barto, 2018). 금융 트레이딩은 순차적 의사결정 문제로 볼 수 있으며, 현재의 행동이 미래의 수익과 위험에 영향을 미치므로 강화학습을 적용하기에 적합한 분야로 간주된다.

초기 연구들은 Q-러닝(Q-learning)이나 SARSA(State-Action-Reward-State-Action)와 같은 테이블 기반의 전통적인 RL 알고리즘을 활용하여 금융 시장에 적용하려는 시도가 있었다 (Neuneier, 1997). 그러나 이러한 접근법은 상태 공간(state space)이나 행동 공간(action space)의 차원이 커질수록 계산량과 메모리 요구량이 기하급수적으로 증가하는 "차원의 저주(curse of dimensionality)" 문제에 직면하여, 실제 복잡한 금융 시장 데이터에 적용하는 데 한계를 보였다.

딥러닝(Deep Learning)과 강화학습을 결합한 심층 강화학습(Deep Reinforcement Learning, DRL)의 발전은 이러한 한계를 극복하고 고차원 금융 데이터를 효과적으로 처리할 수 있는 길을 열었다. 특히, Mnih et al. (2015)이 제안한 심층 Q-네트워크(Deep Q-Network, DQN)는 컨볼루션 신경망을 사용하여 고차원 시각적 입력(예: Atari 게임 화면)으로부터 직접 가치 함수를 근사하는 데 성공했으며, 이는 금융 시계열 데이터 분석에도 영감을 주었다. 이후, Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) 및 Proximal Policy Optimization (PPO) (Schulman et al., 2017)과 같은 더 발전된 DRL 알고리즘들이 등장하여, 복잡하고 변동성이 큰 금융 시장 환경에서도 비교적 안정적인 학습 성능을 보이며 주목받고 있다. 이들 알고리즘은 주식 거래, 포트폴리오 관리, 옵션 가격결정 등 다양한 금융 문제에 적용되고 있다 (Deng et al., 2017; Xiong et al., 2018; Liang et al., 2018).

**3. 연구 방법론: 동적 앙상블 강화학습 트레이딩 시스템**

본 연구에서 제안하는 동적 앙상블 강화학습 트레이딩 시스템은 [그림 1]과 같이 여러 주요 모듈로 구성된다. 각 모듈은 유기적으로 연동되어 시장 상황을 분석하고 최적의 투자 결정을 내린다. (OCR 이미지의 그림 1과 유사하게 연구 프레임워크를 개념적으로 설명. 본 문서에 실제 그림은 포함되지 않음.)

**3.1. 시스템 전체 아키텍처**
*   **데이터 입력 및 전처리 모듈 (Data Input and Preprocessing Module)**: 금융 시장 데이터(본 연구에서는 비트코인 캔들차트 데이터: 시가(Open), 고가(High), 저가(Low), 종가(Close), 거래량(Volume) - OHLCV)와 시장 관련 뉴스 데이터를 수집한다. 수집된 데이터는 결측치 처리, 정규화, 특징 스케일링 등 필요한 전처리 과정을 거친다.
*   **멀티모달 특징 추출 모듈 (Multimodal Feature Extraction Module)**: 다양한 유형의 데이터에서 트레이딩 결정에 유용한 특징을 추출한다. (캔들차트 CNN 특징, 기술적 지표, 뉴스 감성 점수 등)
*   **시장 레짐 분류 모델 (Market Regime Classification Model)**: 멀티모달 특징 추출 모듈에서 추출된 특징 벡터를 입력으로 받아, 현재 시장 상태를 상승장(bull), 하락장(bear), 또는 횡보장(sideways) 중 하나로 분류한다. 본 연구에서는 XGBoost를 주요 분류기로 활용하였다.
*   **레짐별 특화 강화학습 에이전트 풀 (Regime-Specific Reinforcement Learning Agent Pool)**: 각 시장 레짐에 최적화된 성능을 보이도록 개별적으로 학습된 강화학습 에이전트들로 구성된다. (상승장, 하락장, 횡보장 특화 에이전트)
*   **동적 앙상블 및 의사결정 모듈 (Dynamic Ensemble and Decision-Making Module)**: 시장 레짐 분류 모델의 실시간 분류 결과와 각 에이전트의 최근 성과, 에이전트 간 예측의 상관관계 등을 종합적으로 고려하여, 레짐별 특화 에이전트 풀에서 최적의 에이전트 조합을 동적으로 선택하고 각 에이전트에 가중치를 할당한다. 이 가중치를 바탕으로 개별 에이전트들의 투자 의견(action)을 결합하여 최종 투자 행동(매수, 매도, 홀드)을 결정한다.

**3.2. 시장 레짐 분류 방법론**
시장 레짐을 효과적으로 분류하기 위해, 본 연구에서는 규칙 기반 레이블링(rule-based labeling)과 지도학습(supervised learning)을 결합한 하이브리드 접근법을 사용한다.

**3.2.1. 레짐 레이블링 절차**
역사적 시계열 데이터에서 시장 레짐(상승, 하락, 횡보)을 식별하기 위해, 첫 단계로 규칙 기반 레이블링을 수행한다. 예를 들어, 특정 기간 동안의 주가 이동평균선의 기울기, 주가의 장기 추세 대비 상대적 위치, 변동성 지표 등을 종합적으로 고려하여 각 시점의 레짐을 정의한다. 이 과정에는 금융 전문가의 지식이나 경험이 반영될 수 있으며, 다양한 시장 상황에 대한 대표성을 확보하는 것이 중요하다. (예: 상승장은 20일 지수이동평균(EMA)이 50일 EMA 위에 있고, 50일 EMA가 200일 EMA 위에 있는 경우 등으로 정의).

**3.2.2. 레짐 분류 모델 설계**
규칙 기반으로 레이블링된 데이터를 사용하여 지도학습 기반의 시장 레짐 분류 모델을 학습시킨다. 입력 특징으로는 3.1절에서 언급된 멀티모달 특징(캔들차트 CNN 특징, 기술적 지표, 뉴스 감성 점수)을 사용한다. 분류 모델로는 랜덤 포레스트, 그래디언트 부스팅 머신, 또는 다층 퍼셉트론(Multi-Layer Perceptron, MLP)과 같은 다양한 기계학습 알고리즘을 고려할 수 있으며, 교차 검증을 통해 최적의 모델과 하이퍼파라미터를 선택한다. 본 연구에서는 XGBoost를 주요 분류기로 활용하였다.

**3.3. 강화학습 에이전트 설계**
각 시장 레짐에 특화된 강화학습 에이전트들은 기본적으로 동일한 심층 강화학습 알고리즘 아키텍처를 공유하되, 해당 레짐의 특성에 맞게 보상 함수(reward function)와 주요 하이퍼파라미터가 세밀하게 조정된다.

**3.3.1. MDP (Markov Decision Process) 정의**
강화학습 문제는 다음과 같은 마르코프 결정 과정(MDP)으로 정의된다: $M = (S, A, P, R, \gamma)$
*   **상태 공간 (State Space, $S$)**: 에이전트가 의사결정을 내리는 데 사용하는 정보의 집합으로, 다차원 상태 벡터 $s_t \in S$로 구성된다. (멀티모달 특징 벡터, 현재 포지션 정보, 포트폴리오 상태, 최근 행동 이력, 최근 수익률 등)
*   **행동 공간 (Action Space, $A$)**: 에이전트가 각 타임스텝 $t$에서 취할 수 있는 이산적인 행동의 집합 $a_t \in A$. (매수_강, 매수_약, 홀드, 매도_약, 매도_강 등)
*   **상태 전이 확률 (State Transition Probability, $P$)**: $P(s_{t+1} | s_t, a_t)$. 모델-프리 RL 접근법을 사용하므로 명시적으로 알 필요는 없다.
*   **보상 함수 (Reward Function, $R$)**: $r_t = R(s_t, a_t, s_{t+1})$. 레짐별로 차별화된 보상 함수 설계. (기본 수익 구성 요소, 위험 조정 항, 거래 비용 페널티, 레짐별 조정 항 등)
*   **할인 계수 (Discount Factor, $\gamma$)**: $\gamma = 0.95$와 같이 비교적 높은 값을 설정하여 중장기적인 누적 수익을 고려.

**3.3.2. 에이전트 알고리즘 구현**
DQN (Deep Q-Network) 및 PPO (Proximal Policy Optimization) 알고리즘을 기반으로 각 레짐별 특화 에이전트를 구성한다. DQN은 주로 상승장 특화 에이전트로, PPO는 변동성이 높고 빠른 적응이 필요한 하락장이나 횡보장 특화 에이전트로 활용된다. 각 알고리즘은 해당 레짐의 특성에 맞게 하이퍼파라미터(학습률, 할인 계수 등)를 조정한다.

**3.4. 동적 앙상블 메커니즘**
시장 레짐 분류 결과와 각 에이전트의 특성을 고려하여, 레짐별 특화 에이전트들의 의견을 효과적으로 결합하는 핵심 컴포넌트이다.

**3.4.1. 에이전트 선택 및 가중치 할당 방법**
에이전트 가중치 $w_{(i,t)}$는 레짐 기반 가중치 할당, 성과 기반 가중치 조정, 상관관계 기반 조정 등을 복합적으로 고려하여 결정된다.
*   레짐 기반 가중치: $w_{(i,t)}^{base} = P(R_t = r_i^{spec}) \times \beta_i$
*   상관관계 조정 가중치: $w_{(i,t)}^{corr} = 1 - \frac{1}{(k-1)}\sum_{j \neq i} \rho_{(i,j,t)}$

**3.4.2. 최종 행동 결정 방법**
가중 평균 Q-값 방식, 가중 투표 방식, 신뢰도 임계치 방식 등을 사용하거나 이들을 하이브리드 형태로 결합하여 최종 행동을 결정한다.
*   가중 평균 Q-값: $Q_{ensemble}(s_t, a) = \sum_{i=1}^{k} w_{(i,t)} \times Q_i(s_t, a)$, $a_t^{ensemble} = \arg\max_a Q_{ensemble}(s_t, a)$
*   신뢰도 임계치: $a_t^{ensemble} = a_{i^*}$ if $w_{(i^*,t)} > \tau$, else Hold.

**3.5. 학습 및 최적화 과정**
*   **데이터 분할**: 학습(2019.01~2021.06), 검증(2021.07~2022.06), 테스트(2022.07~2023.12) 데이터로 분할.
*   **하이퍼파라미터 최적화**: 베이지안 최적화 등을 사용하여 검증 데이터에서 샤프 비율을 최대화하는 하이퍼파라미터(학습률, 네트워크 크기 등)를 탐색.

**4. 실험 설계 (Experimental Design)**

**4.1. 실험 데이터셋**
*   **금융 시계열 데이터**: 비트코인(BTC/USDT) 2019년 1월 1일 ~ 2023년 12월 31일 일봉/시간봉 OHLCV 데이터 (Binance 등).
*   **뉴스 데이터**: CryptoCompare API, CoinDesk 등 주요 암호화폐 뉴스 플랫폼에서 동일 기간 뉴스 기사(헤드라인, 본문, 발행 시간) 수집. DeepSeek R1 언어 모델로 -1~+1 범위 감성 점수 산출.

**4.2. 비교 대상 모델 (Benchmarks)**
*   **정적 멀티모달 강화학습 (Static Multimodal RL)**: 시장 레짐 구분 없이 멀티모달 데이터를 사용하는 단일 RL 에이전트.
*   **전통적 트레이딩 전략**: 이동평균 교차 전략 (10일/30일), MACD 전략, RSI 기반 전략 (RSI<30 매수, RSI>70 매도).
*   **Buy & Hold 전략**: 테스트 기간 시작 시 전액 매수 후 보유.

**4.3. 평가 지표 (Evaluation Metrics)**
*   **수익성 지표**: 누적 수익률, 연평균 수익률(CAGR), 월별 수익률 분포, 승률.
*   **리스크 조정 수익성 지표**: 샤프 비율(SR), 소티노 비율(SoR), 칼마 비율(CR_Calmar).
*   **위험 지표**: 최대 낙폭(MDD), 일별/월별 변동성, 다운사이드 편차.
*   **거래 관련 지표**: 평균 거래 횟수, 거래 비용.

**5. 실험 결과 및 분석 (Results and Discussion)**

**5.1. 시장 레짐 분류 성능**
제안된 시장 레짐 분류 모델(XGBoost 사용)은 테스트 데이터셋(2022년 7월 ~ 2023년 12월)에서 전체 평균 84.5%의 정확도를 달성했다. 상승장(88.5%)과 하락장(86.1%) 분류 정확도가 횡보장(79.0%)보다 높게 나타났다. 횡보장은 명확한 방향성이 없어 특징 파악이 더 어려운 것으로 분석된다.

**5.2. 트레이딩 성과 비교 분석**
제안된 동적 앙상블 강화학습 시스템은 테스트 데이터셋에서 모든 비교 대상 모델보다 우수한 성과를 보였다. 특히 정적 멀티모달 RL 시스템 대비 누적 수익률은 약 18.8%p, 샤프 비율은 약 0.55, MDD는 약 6.4%p 개선되었다. Buy & Hold 전략 및 전통적 기술 지표 기반 전략들보다도 현저히 나은 위험 조정 수익성을 나타냈다. 하락장 구간(예: 2022년)에서도 제안 시스템은 +7.9%의 긍정적 수익률을 기록하여 뛰어난 위험 관리 능력을 입증했다. 또한, 레짐 판단을 통해 불필요한 거래를 줄임으로써 정적 RL 시스템 대비 약 12-15%의 거래 비용 절감 효과를 보였다. 이러한 결과는 제안 시스템이 다양한 시장 레짐 변화에 강건하게 대응하며 수익성, 위험 관리, 거래 효율성 모든 측면에서 우수함을 뒷받침한다.

**6. 결론 및 향후 연구방향 (Conclusion and Future Work)**

본 연구에서는 금융 시장의 다양한 레짐 변화에 효과적으로 대응하고 장기적으로 안정적인 수익을 추구할 수 있는 동적 앙상블 강화학습 트레이딩 시스템을 제안하고 그 성능을 검증하였다. 제안 시스템은 멀티모달 데이터(캔들차트, 기술적 지표, 뉴스 감성)를 종합적으로 활용하여 현재 시장을 상승장, 하락장, 횡보장의 세 가지 레짐으로 실시간 분류한다. 이후, 각 레짐에 특화되어 사전 학습된 강화학습 에이전트들(DQN, PPO 등)로 구성된 풀에서, 시장 레짐 분류 결과, 에이전트의 최근 성과, 그리고 에이전트 간 상관관계를 고려한 동적 가중치 할당 메커니즘을 통해 최적의 에이전트 조합을 구성하고 최종 투자 결정을 내린다.

비트코인(BTC/USDT) 시계열 및 뉴스 데이터를 사용한 백테스팅 결과, 제안된 동적 앙상블 시스템은 시장 레짐을 고려하지 않는 정적 멀티모달 강화학습 시스템 대비 누적 수익률에서 약 18.8%p 향상, 샤프 비율에서 약 0.55 개선, 그리고 최대 낙폭(MDD)에서 약 6.4%p 감소라는 통계적으로 유의미한 성과 개선을 보였다. 특히, 시장의 변동성이 크거나 하락 추세가 나타난 구간에서도 손실을 효과적으로 방어하고 긍정적인 수익률(예: 2022년 하락장에서 +7.9%)을 달성하는 등, 제안 시스템의 우수한 위험 관리 능력과 시장 적응성을 확인할 수 있었다. 또한, 평균 84.5%의 시장 레짐 감지 정확도를 바탕으로 불필요한 거래를 줄여 약 12-15%의 거래 비용 절감 효과도 관찰되었다.

본 연구는 다음과 같은 학술적 및 실무적 기여를 한다. 학술적으로는 시장 레짐을 명시적으로 고려한 강화학습 프레임워크를 확장하고, 시장 상황, 에이전트 성과, 상관관계를 고려한 동적 앙상블 메커니즘을 금융 분야에 제안하였다. 실무적으로는 시장 레짐에 따라 투자 강도와 방향을 조절하는 유연한 자산 배분 전략의 가능성을 제시하고, 뉴스 감성 분석이 트레이딩 성능 향상에 기여함을 보여 정량적 분석과 정성적 분석을 결합한 하이브리드 투자의 가능성을 높였다.

본 연구의 한계점으로는 데이터 범위 및 자산 클래스 제한, 계산 복잡성, 시장 레짐 정의 및 레이블링의 주관성, 모델 복잡성 대비 해석 가능성 부족 등이 있다.

향후 연구 방향으로는 첫째, 설명 가능한 AI(XAI) 기법을 통합하여 모델 결정의 투명성을 높이는 연구가 필요하다. 둘째, 실시간 데이터로 모델을 지속적으로 미세 조정하는 온라인 학습 방식을 도입하여 시장 변화에 대한 적응력을 강화할 수 있다. 셋째, 현재 레짐 분류에서 나아가 레짐 전환 시점을 예측하는 모델 개발로 선제적 대응 능력을 향상시킬 수 있다. 넷째, 제안 시스템을 다중 자산 포트폴리오 최적화 문제로 확장하는 연구가 가능하다. 마지막으로, 강화학습 기반 자동화 의사결정과 인간 트레이더의 직관을 결합한 하이브리드 의사결정 시스템으로 발전시키는 연구를 고려할 수 있다.

결론적으로, 본 연구는 시장 레짐 변화라는 금융 시장의 근본적인 도전에 대응하기 위한 강건하고 적응적인 동적 앙상블 강화학습 트레이딩 시스템을 제안하고 그 유효성을 입증하였다. 이는 향후 금융공학 및 인공지능 분야에서 더욱 정교하고 실용적인 지능형 투자 시스템 개발을 위한 중요한 이론적, 방법론적 토대를 마련했다는 점에서 의의가 있다.

---
**참고문헌 (References)**

**[국내 문헌]**
*김은미 (2021), 이모세 & 안현철 (2018), 장성일 & 김정연 (2017), 홍태호 외 (2023) 등)*

**[국외 문헌]**
*(Ang & Bekaert (2002), Deng et al. (2017), Dietterich (2000), Fabozzi & Markowitz (2011), Gu et al. (2020), Hamilton (1989), Kearney & Liu (2014), Kritzman et al. (2012), Kuncheva (2004), Liang et al. (2018), Mnih et al. (2015, 2016), Neuneier (1997), Schulman et al. (2017), Sutton & Barto (2018), Timmermann (2006), Xiong et al. (2018), Van Hasselt et al. (2016), Wang et al. (2016), Wolpert (1992) 등을 포함하여 알파벳 순으로 정리합니다.)*

---

**Abstract (영문 초록)**

This study proposes a robust dynamic ensemble reinforcement learning trading system designed to effectively respond to the inherent non-stationarity and diverse regime shifts in financial markets, aiming for long-term stable profit generation. Traditional studies have focused on regime identification itself or have shown limitations in effectively translating identified regime information into trading strategies. The proposed system utilizes multimodal data (candlestick charts, technical indicators, news sentiment scores) to classify market regimes in real-time and constructs a pool of reinforcement learning agents specialized for each regime (bull, bear, sideways). Subsequently, a dynamic ensemble mechanism allocates optimal agent combinations and weights based on market conditions to make final investment decisions. Backtesting results using Bitcoin (BTC/USDT) time-series data (2019-2023) showed that the proposed dynamic ensemble system statistically significantly outperformed a static multimodal system in key performance indicators, including an increase in Sharpe Ratio by approximately 0.4-0.6, a reduction in Maximum Drawdown (MDD) by about 7-9 percentage points, and an improvement in Win Rate by around 5-8 percentage points. Furthermore, it achieved an average market regime detection accuracy of 84.5%, thereby reducing unnecessary trades and cutting transaction costs by approximately 12-15%. This research is expected to enhance adaptability to market regime changes and improve the practicality of reinforcement learning-based trading systems.

**Key Words**: Reinforcement Learning, Dynamic Ensemble, Market Regime, Multimodal Learning, Financial Time Series Analysis, Trading System

---
**저자소개**

**허상훈 (Sanghun Heo)**
*   홍익대학교에서 산업디자인학 석사학위를 취득하였으며, 현재 충북대학교 산업인공지능학과 박사과정에 재학 중이다. 주요 연구 분야는 금융 시계열 예측, 멀티모달 학습, 딥러닝 및 강화학습 기반의 암호화폐 자동매매 전략 개발이다. 특히, 캔들차트 이미지와 뉴스 감성 분석을 결합한 멀티모달 예측 모델을 통해 시장 변동성에 효과적으로 대응하는 연구를 수행하고 있다.

**황영배 (Youngbae Hwang)**
*   現 충북대학교 산업인공지능학과 교수. 

---
**(페이지 번호, 머리글/바닥글 등은 실제 학술지 양식에 맞춰 추가되어야 합니다.)**